{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6c6177",
   "metadata": {},
   "source": [
    "# 목차\n",
    "## 1. ChatGPT Model Analysis\n",
    "## 2. Meta LLama Open Source Analysis\n",
    "## 3. Meta LLama Open Source Running Exeample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265dd1a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212a8c2",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab0d91",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f724e7a1",
   "metadata": {},
   "source": [
    "# 1. ChatGPT Model Analsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6375441",
   "metadata": {},
   "source": [
    "* ## 1-1 개요\n",
    "    * ### Base Model : GPT 3.5\n",
    "    <br>\n",
    "    * ### 기술 요소\n",
    "        * ###  RLHF(Reinforcement Learning by Human Feedback)\n",
    "            * #### SFT(Supervised Fine-Tuning)\n",
    "            * #### RM(Reword Model)\n",
    "            * #### PPO(Proximal Policy Optimization) Algorithm\n",
    "        * ### 대화형 에이전트에 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385afc55",
   "metadata": {},
   "source": [
    "* ## 1-2 학습 과정\n",
    "<img src = \"https://www.atriainnovation.com/wp-content/uploads/2023/01/Diagrama-980x538.jpg.webp\" width=\"750px\" height=\"500px\"></img>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad558b05",
   "metadata": {},
   "source": [
    "* ## Step 1. SFT(Supervised Fine-Tuning Model)\n",
    "    * #### 지도학습으로 Fine-Tuning 모델\n",
    "    * #### 사람이 Prompts Dataset을 만듬\n",
    "    * #### 고품질 큐레이션 데이터 셋(약 12 ~ 15만 개 데이터로 추정)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* ## Step 2. Reward Model(RM)\n",
    "    * #### 질문이 들어오면 위의 SFT Model이 질문에 대한 여러개의 출력을 생성한다.\n",
    "    * #### 그리고 사람이 출력된 문장에 랭킹을 매겨 라벨링하여 새 데이터 셋을 생성한다.\n",
    "    * #### 새로운 데이터 셋으로 Reword Model을 학습시킨다.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* ## Step 3. PPO(Proximal Policy Optimization) Reinforcement Learning Algorithm\n",
    "    * #### Step 1의 SFT 모델과 Step 2의 Reward Model을 사용해 강화학습\n",
    "    * #### PPO 사용\n",
    "        * ##### Reward Model을 보상함수로 사용하여 정책을 최적화\n",
    "    * #### 강화학습을 통한 fine-tuning\n",
    "        * ##### SFT 모델에 프롬프트를 넣어, 그 결과를 추론\n",
    "        * ##### 결과를 Reward Model에 넣어 평가하여 보상을 계산\n",
    "        * ##### 보상 값을 다시 SFT 모델에게 주어지고, 모델은 정책을 업데이트함\n",
    "        * ##### 그 결과 사람이 원하는 아웃풋에 더 가까운 결과를 냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b83d49",
   "metadata": {},
   "source": [
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbBynJk%2FbtqWQ8Wztuu%2FPHCSlHY0QYrXOugdOB2Ldk%2Fimg.png\" width=\"500\" height=\"500px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99ec4f6c",
   "metadata": {},
   "source": [
    "# 2. Meta Open Source LLama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41eddba",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc287e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    "    ColumnParallelLinear,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // fs_init.get_model_parallel_world_size()\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wk = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wv = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wo = RowParallelLinear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "            input_is_parallel=True,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "        self.w2 = RowParallelLinear(\n",
    "            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n",
    "        )\n",
    "        self.w3 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = ParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits\n",
    "        return output.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a4ea8",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from logging import getLogger\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        # reload tokenizer\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n",
    "\n",
    "        # BOS / EOS token IDs\n",
    "        self.n_words: int = self.sp_model.vocab_size()\n",
    "        self.bos_id: int = self.sp_model.bos_id()\n",
    "        self.eos_id: int = self.sp_model.eos_id()\n",
    "        self.pad_id: int = self.sp_model.pad_id()\n",
    "        logger.info(\n",
    "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
    "        )\n",
    "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "        assert type(s) is str\n",
    "        t = self.sp_model.encode(s)\n",
    "        if bos:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self.sp_model.decode(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114f2f0",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ba86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "\n",
    "from .tokenizer import Tokenizer\n",
    "from .model import Transformer\n",
    "\n",
    "\n",
    "class LLaMA:\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> List[str]:\n",
    "        bsz = len(prompts)\n",
    "        params = self.model.params\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "        min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "        max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n",
    "        \n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "            \n",
    "        input_text_mask = tokens != self.tokenizer.pad_id\n",
    "        start_pos = min_prompt_size\n",
    "        prev_pos = 0\n",
    "        \n",
    "        # 문장 생성\n",
    "        \n",
    "        for cur_pos in range(start_pos, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            \n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            prev_pos = cur_pos\n",
    "        \n",
    "        decoded = []\n",
    "        for i, t in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            try:\n",
    "                t = t[: t.index(self.tokenizer.eos_id)]\n",
    "            except ValueError:\n",
    "                pass\n",
    "            decoded.append(self.tokenizer.decode(t))\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283cc73",
   "metadata": {},
   "source": [
    "## Exeample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299390a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from typing import Tuple\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import fire\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "\n",
    "\n",
    "def setup_model_parallel() -> Tuple[int, int]:\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n",
    "\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    initialize_model_parallel(world_size)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # seed must be the same in all processes\n",
    "    torch.manual_seed(1)\n",
    "    return local_rank, world_size\n",
    "\n",
    "\n",
    "def load(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    local_rank: int,\n",
    "    world_size: int,\n",
    "    max_seq_len: int,\n",
    "    max_batch_size: int,\n",
    ") -> LLaMA:\n",
    "    start_time = time.time() # 시간체크\n",
    "    \n",
    "    ### Model CheckPoint 설정\n",
    "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "    assert world_size == len(\n",
    "        checkpoints\n",
    "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
    "    ckpt_path = checkpoints[local_rank]\n",
    "    \n",
    "    \n",
    "    print(\"Loading\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    \n",
    "    # 하이퍼 파라메타 적용\n",
    "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f: # META의 HParam\n",
    "        params = json.loads(f.read())\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = Tokenizer(model_path=tokenizer_path) # META에서 제공하는Tokenizer \n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    \n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    \n",
    "    model = Transformer(model_args)                  # META에서 제공하는 Model\n",
    "    \n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    generator = LLaMA(model, tokenizer) # LLama 파이프라인\n",
    "    print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    return generator\n",
    "\n",
    "\n",
    "def main(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.95,\n",
    "    max_seq_len: int = 512,\n",
    "    max_batch_size: int = 32,\n",
    "):\n",
    "    local_rank, world_size = setup_model_parallel()\n",
    "    if local_rank > 0:\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "        \n",
    "    generator = load(\n",
    "        ckpt_dir, tokenizer_path, local_rank, world_size, max_seq_len, max_batch_size\n",
    "    )\n",
    "    # Input Data\n",
    "    prompts = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "        \"I believe the meaning of life is\",\n",
    "        \"Simply put, the theory of relativity states that \",\n",
    "        \"Building a website can be done in 10 simple steps:\\n\",\n",
    "        # Few shot prompts: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api\n",
    "        \"\"\"Tweet: \"I hate it when my phone battery dies.\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"My day has been 👍\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"This is the link to the article\"\n",
    "Sentiment: Neutral\n",
    "###\n",
    "Tweet: \"This new music video was incredibile\"\n",
    "Sentiment:\"\"\",\n",
    "        \"\"\"Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "\n",
    "peppermint => menthe poivrée\n",
    "\n",
    "plush girafe => girafe peluche\n",
    "\n",
    "cheese =>\"\"\",\n",
    "    ]\n",
    "    results = generator.generate(\n",
    "        prompts, max_gen_len=256, temperature=temperature, top_p=top_p\n",
    "    )\n",
    "\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(\"\\n==================================\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdd981",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a3cb7",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d5b63",
   "metadata": {},
   "source": [
    "# 3. Chat LLama Open Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616b606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b3d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd6100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c477bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5a84751",
   "metadata": {},
   "source": [
    "* ### github 주소\n",
    "    * #### LLama : https://github.com/facebookresearch/llama\n",
    "    * #### LLama Model Download : https://github.com/shawwn/llama-dl\n",
    "    * #### ChatLLama : https://github.com/juncongmoo/chatllama\n",
    "    * #### picoGPT : https://github.com/jaymody/picoGPT\n",
    "    * #### Alpaca : https://github.com/tatsu-lab/stanford_alpaca\n",
    "    * #### picoGPT실행 : https://colab.research.google.com/drive/1zNzuTJJt5nqmhQY_qTn1pkibThkmzS2S?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69755b5c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fde85",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6c6177",
   "metadata": {},
   "source": [
    "# ëª©ì°¨\n",
    "## 1. ChatGPT Model Analysis\n",
    "## 2. Meta LLama Open Source Analysis\n",
    "## 3. Meta LLama Open Source Running Exeample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c265dd1a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f212a8c2",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab0d91",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f724e7a1",
   "metadata": {},
   "source": [
    "# 1. ChatGPT Model Analsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6375441",
   "metadata": {},
   "source": [
    "* ## 1-1 ê°œìš”\n",
    "    * ### Base Model : GPT 3.5\n",
    "    <br>\n",
    "    * ### ê¸°ìˆ  ìš”ì†Œ\n",
    "        * ###  RLHF(Reinforcement Learning by Human Feedback)\n",
    "            * #### SFT(Supervised Fine-Tuning)\n",
    "            * #### RM(Reword Model)\n",
    "            * #### PPO(Proximal Policy Optimization) Algorithm\n",
    "        * ### ëŒ€í™”í˜• ì—ì´ì „íŠ¸ì— ìµœì í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385afc55",
   "metadata": {},
   "source": [
    "* ## 1-2 í•™ìŠµ ê³¼ì •\n",
    "<img src = \"https://www.atriainnovation.com/wp-content/uploads/2023/01/Diagrama-980x538.jpg.webp\" width=\"750px\" height=\"500px\"></img>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad558b05",
   "metadata": {},
   "source": [
    "* ## Step 1. SFT(Supervised Fine-Tuning Model)\n",
    "    * #### ì§€ë„í•™ìŠµìœ¼ë¡œ Fine-Tuning ëª¨ë¸\n",
    "    * #### ì‚¬ëŒì´ Prompts Datasetì„ ë§Œë“¬\n",
    "    * #### ê³ í’ˆì§ˆ íë ˆì´ì…˜ ë°ì´í„° ì…‹(ì•½ 12 ~ 15ë§Œ ê°œ ë°ì´í„°ë¡œ ì¶”ì •)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* ## Step 2. Reward Model(RM)\n",
    "    * #### ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ ìœ„ì˜ SFT Modelì´ ì§ˆë¬¸ì— ëŒ€í•œ ì—¬ëŸ¬ê°œì˜ ì¶œë ¥ì„ ìƒì„±í•œë‹¤.\n",
    "    * #### ê·¸ë¦¬ê³  ì‚¬ëŒì´ ì¶œë ¥ëœ ë¬¸ì¥ì— ë­í‚¹ì„ ë§¤ê²¨ ë¼ë²¨ë§í•˜ì—¬ ìƒˆ ë°ì´í„° ì…‹ì„ ìƒì„±í•œë‹¤.\n",
    "    * #### ìƒˆë¡œìš´ ë°ì´í„° ì…‹ìœ¼ë¡œ Reword Modelì„ í•™ìŠµì‹œí‚¨ë‹¤.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* ## Step 3. PPO(Proximal Policy Optimization) Reinforcement Learning Algorithm\n",
    "    * #### Step 1ì˜ SFT ëª¨ë¸ê³¼ Step 2ì˜ Reward Modelì„ ì‚¬ìš©í•´ ê°•í™”í•™ìŠµ\n",
    "    * #### PPO ì‚¬ìš©\n",
    "        * ##### Reward Modelì„ ë³´ìƒí•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ì—¬ ì •ì±…ì„ ìµœì í™”\n",
    "    * #### ê°•í™”í•™ìŠµì„ í†µí•œ fine-tuning\n",
    "        * ##### SFT ëª¨ë¸ì— í”„ë¡¬í”„íŠ¸ë¥¼ ë„£ì–´, ê·¸ ê²°ê³¼ë¥¼ ì¶”ë¡ \n",
    "        * ##### ê²°ê³¼ë¥¼ Reward Modelì— ë„£ì–´ í‰ê°€í•˜ì—¬ ë³´ìƒì„ ê³„ì‚°\n",
    "        * ##### ë³´ìƒ ê°’ì„ ë‹¤ì‹œ SFT ëª¨ë¸ì—ê²Œ ì£¼ì–´ì§€ê³ , ëª¨ë¸ì€ ì •ì±…ì„ ì—…ë°ì´íŠ¸í•¨\n",
    "        * ##### ê·¸ ê²°ê³¼ ì‚¬ëŒì´ ì›í•˜ëŠ” ì•„ì›ƒí’‹ì— ë” ê°€ê¹Œìš´ ê²°ê³¼ë¥¼ ëƒ„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b83d49",
   "metadata": {},
   "source": [
    "<img src = \"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbBynJk%2FbtqWQ8Wztuu%2FPHCSlHY0QYrXOugdOB2Ldk%2Fimg.png\" width=\"500\" height=\"500px\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f716a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99ec4f6c",
   "metadata": {},
   "source": [
    "# 2. Meta Open Source LLama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41eddba",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc287e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import fairscale.nn.model_parallel.initialize as fs_init\n",
    "from fairscale.nn.model_parallel.layers import (\n",
    "    ParallelEmbedding,\n",
    "    RowParallelLinear,\n",
    "    ColumnParallelLinear,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_local_heads = args.n_heads // fs_init.get_model_parallel_world_size()\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wk = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wv = ColumnParallelLinear(\n",
    "            args.dim,\n",
    "            args.n_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            gather_output=False,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "        self.wo = RowParallelLinear(\n",
    "            args.n_heads * self.head_dim,\n",
    "            args.dim,\n",
    "            bias=False,\n",
    "            input_is_parallel=True,\n",
    "            init_method=lambda x: x,\n",
    "        )\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_local_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "        self.w2 = RowParallelLinear(\n",
    "            hidden_dim, dim, bias=False, input_is_parallel=True, init_method=lambda x: x\n",
    "        )\n",
    "        self.w3 = ColumnParallelLinear(\n",
    "            dim, hidden_dim, bias=False, gather_output=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = ParallelEmbedding(\n",
    "            params.vocab_size, params.dim, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = ColumnParallelLinear(\n",
    "            params.dim, params.vocab_size, bias=False, init_method=lambda x: x\n",
    "        )\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits\n",
    "        return output.float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a4ea8",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "from logging import getLogger\n",
    "from typing import List\n",
    "import os\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, model_path: str):\n",
    "        # reload tokenizer\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "        self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "        logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n",
    "\n",
    "        # BOS / EOS token IDs\n",
    "        self.n_words: int = self.sp_model.vocab_size()\n",
    "        self.bos_id: int = self.sp_model.bos_id()\n",
    "        self.eos_id: int = self.sp_model.eos_id()\n",
    "        self.pad_id: int = self.sp_model.pad_id()\n",
    "        logger.info(\n",
    "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
    "        )\n",
    "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "        assert type(s) is str\n",
    "        t = self.sp_model.encode(s)\n",
    "        if bos:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        return self.sp_model.decode(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114f2f0",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ba86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "\n",
    "from .tokenizer import Tokenizer\n",
    "from .model import Transformer\n",
    "\n",
    "\n",
    "class LLaMA:\n",
    "    def __init__(self, model: Transformer, tokenizer: Tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        max_gen_len: int,\n",
    "        temperature: float = 0.8,\n",
    "        top_p: float = 0.95,\n",
    "    ) -> List[str]:\n",
    "        bsz = len(prompts)\n",
    "        params = self.model.params\n",
    "        assert bsz <= params.max_batch_size, (bsz, params.max_batch_size)\n",
    "\n",
    "        prompt_tokens = [self.tokenizer.encode(x, bos=True, eos=False) for x in prompts]\n",
    "\n",
    "        min_prompt_size = min([len(t) for t in prompt_tokens])\n",
    "        max_prompt_size = max([len(t) for t in prompt_tokens])\n",
    "\n",
    "        total_len = min(params.max_seq_len, max_gen_len + max_prompt_size)\n",
    "\n",
    "        tokens = torch.full((bsz, total_len), self.tokenizer.pad_id).cuda().long()\n",
    "        \n",
    "        for k, t in enumerate(prompt_tokens):\n",
    "            tokens[k, : len(t)] = torch.tensor(t).long()\n",
    "            \n",
    "        input_text_mask = tokens != self.tokenizer.pad_id\n",
    "        start_pos = min_prompt_size\n",
    "        prev_pos = 0\n",
    "        \n",
    "        # ë¬¸ì¥ ìƒì„±\n",
    "        \n",
    "        for cur_pos in range(start_pos, total_len):\n",
    "            logits = self.model.forward(tokens[:, prev_pos:cur_pos], prev_pos)\n",
    "            \n",
    "            if temperature > 0:\n",
    "                probs = torch.softmax(logits / temperature, dim=-1)\n",
    "                next_token = sample_top_p(probs, top_p)\n",
    "            else:\n",
    "                next_token = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            next_token = next_token.reshape(-1)\n",
    "            # only replace token if prompt has already been generated\n",
    "            next_token = torch.where(\n",
    "                input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "            )\n",
    "            tokens[:, cur_pos] = next_token\n",
    "            prev_pos = cur_pos\n",
    "        \n",
    "        decoded = []\n",
    "        for i, t in enumerate(tokens.tolist()):\n",
    "            # cut to max gen len\n",
    "            t = t[: len(prompt_tokens[i]) + max_gen_len]\n",
    "            # cut to eos tok if any\n",
    "            try:\n",
    "                t = t[: t.index(self.tokenizer.eos_id)]\n",
    "            except ValueError:\n",
    "                pass\n",
    "            decoded.append(self.tokenizer.decode(t))\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def sample_top_p(probs, p):\n",
    "    probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "    probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
    "    mask = probs_sum - probs_sort > p\n",
    "    probs_sort[mask] = 0.0\n",
    "    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "    next_token = torch.multinomial(probs_sort, num_samples=1)\n",
    "    next_token = torch.gather(probs_idx, -1, next_token)\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283cc73",
   "metadata": {},
   "source": [
    "## Exeample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299390a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from typing import Tuple\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import fire\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "from llama import ModelArgs, Transformer, Tokenizer, LLaMA\n",
    "\n",
    "\n",
    "def setup_model_parallel() -> Tuple[int, int]:\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", -1))\n",
    "\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    initialize_model_parallel(world_size)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # seed must be the same in all processes\n",
    "    torch.manual_seed(1)\n",
    "    return local_rank, world_size\n",
    "\n",
    "\n",
    "def load(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    local_rank: int,\n",
    "    world_size: int,\n",
    "    max_seq_len: int,\n",
    "    max_batch_size: int,\n",
    ") -> LLaMA:\n",
    "    start_time = time.time() # ì‹œê°„ì²´í¬\n",
    "    \n",
    "    ### Model CheckPoint ì„¤ì •\n",
    "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "    assert world_size == len(\n",
    "        checkpoints\n",
    "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
    "    ckpt_path = checkpoints[local_rank]\n",
    "    \n",
    "    \n",
    "    print(\"Loading\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    \n",
    "    # í•˜ì´í¼ íŒŒë¼ë©”íƒ€ ì ìš©\n",
    "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f: # METAì˜ HParam\n",
    "        params = json.loads(f.read())\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tokenizer = Tokenizer(model_path=tokenizer_path) # METAì—ì„œ ì œê³µí•˜ëŠ”Tokenizer \n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    \n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    \n",
    "    model = Transformer(model_args)                  # METAì—ì„œ ì œê³µí•˜ëŠ” Model\n",
    "    \n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    generator = LLaMA(model, tokenizer) # LLama íŒŒì´í”„ë¼ì¸\n",
    "    print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    return generator\n",
    "\n",
    "\n",
    "def main(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.95,\n",
    "    max_seq_len: int = 512,\n",
    "    max_batch_size: int = 32,\n",
    "):\n",
    "    local_rank, world_size = setup_model_parallel()\n",
    "    if local_rank > 0:\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "        \n",
    "    generator = load(\n",
    "        ckpt_dir, tokenizer_path, local_rank, world_size, max_seq_len, max_batch_size\n",
    "    )\n",
    "    # Input Data\n",
    "    prompts = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "        \"I believe the meaning of life is\",\n",
    "        \"Simply put, the theory of relativity states that \",\n",
    "        \"Building a website can be done in 10 simple steps:\\n\",\n",
    "        # Few shot prompts: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api\n",
    "        \"\"\"Tweet: \"I hate it when my phone battery dies.\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"My day has been ğŸ‘\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"This is the link to the article\"\n",
    "Sentiment: Neutral\n",
    "###\n",
    "Tweet: \"This new music video was incredibile\"\n",
    "Sentiment:\"\"\",\n",
    "        \"\"\"Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "\n",
    "peppermint => menthe poivrÃ©e\n",
    "\n",
    "plush girafe => girafe peluche\n",
    "\n",
    "cheese =>\"\"\",\n",
    "    ]\n",
    "    results = generator.generate(\n",
    "        prompts, max_gen_len=256, temperature=temperature, top_p=top_p\n",
    "    )\n",
    "\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(\"\\n==================================\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cdd981",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a3cb7",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164d5b63",
   "metadata": {},
   "source": [
    "# 3. Chat LLama Open Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616b606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b3d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfd6100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c477bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5a84751",
   "metadata": {},
   "source": [
    "* ### github ì£¼ì†Œ\n",
    "    * #### LLama : https://github.com/facebookresearch/llama\n",
    "    * #### LLama Model Download : https://github.com/shawwn/llama-dl\n",
    "    * #### ChatLLama : https://github.com/juncongmoo/chatllama\n",
    "    * #### picoGPT : https://github.com/jaymody/picoGPT\n",
    "    * #### Alpaca : https://github.com/tatsu-lab/stanford_alpaca\n",
    "    * #### picoGPTì‹¤í–‰ : https://colab.research.google.com/drive/1zNzuTJJt5nqmhQY_qTn1pkibThkmzS2S?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69755b5c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5fde85",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
